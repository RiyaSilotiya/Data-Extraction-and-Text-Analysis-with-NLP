{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hFh7yq6GvnlU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('Input.xlsx')"
      ],
      "metadata": {
        "id": "c76fec3ovrjC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_folder = 'output_folder/'"
      ],
      "metadata": {
        "id": "6y3WybLIWJZa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Extraction"
      ],
      "metadata": {
        "id": "BYjeEq1kYfcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "\n",
        "def sanitize_filename(filename):\n",
        "    invalid_chars = '/\\\\?%*:|\"<>!'\n",
        "    for char in invalid_chars:\n",
        "        filename = filename.replace(char, '_')\n",
        "    return filename\n",
        "\n",
        "def extract_article_content(url, output_folder='output/'):\n",
        "\n",
        "    response = requests.get(url)\n",
        "    html_content = response.text\n",
        "\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    title = soup.title.text.strip()\n",
        "\n",
        "    sanitized_title = sanitize_filename(title)\n",
        "\n",
        "    body = soup.body\n",
        "    inner_div = None\n",
        "\n",
        "    if body:\n",
        "        outer_wrap = body.find('div', {'id': 'td-outer-wrap', 'class': 'td-theme-wrap'})\n",
        "        if outer_wrap:\n",
        "            article = outer_wrap.find('article')\n",
        "            if article:\n",
        "                td_containers = article.find_all('div', {'class': 'td-container'})\n",
        "                if td_containers:\n",
        "                    inner_div = td_containers[-1]\n",
        "\n",
        "    if inner_div:\n",
        "        inner_div2 = inner_div.find('div', {'class': 'td-pb-row'})\n",
        "        if inner_div2:\n",
        "            inner_div3 = inner_div2.find('div', {'class': 'td-pb-span8 td-main-content', 'role': 'main'})\n",
        "            inner_div4 = inner_div3.find('div', {'class': 'td-ss-main-content'})\n",
        "            inner_div5 = inner_div4.find('div', {'class': 'td-post-content tagdiv-type'})\n",
        "\n",
        "            if inner_div5:\n",
        "                inner_div5_text = inner_div5.get_text(separator='\\n', strip=True)\n",
        "\n",
        "                # output folder\n",
        "                if not os.path.exists(output_folder):\n",
        "                    os.makedirs(output_folder)\n",
        "\n",
        "                url_id = None\n",
        "                if df is not None:\n",
        "                    url_id_series = df[df['URL'] == url]['URL_ID']\n",
        "                    if not url_id_series.empty:\n",
        "                        url_id = url_id_series.iloc[0]\n",
        "\n",
        "                # Save content with URL_ID\n",
        "                if url_id:\n",
        "                    filename = f'{url_id}.txt'\n",
        "                    file_path = os.path.join(output_folder, filename)\n",
        "                    with open(file_path, 'w', encoding='utf-8') as content_file:\n",
        "                        content_file.write(f\"Title: {title}\\n\\n{inner_div5_text}\")\n",
        "\n",
        "                    print(f\"Content extracted for {url} with URL_ID {url_id}\")\n",
        "                    return\n",
        "\n",
        "    print(f\"No content extracted for {url}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "i83NKEevdpw0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df.iterrows():\n",
        "    url = row['URL']\n",
        "    extract_article_content(url, output_folder)"
      ],
      "metadata": {
        "id": "hkBOg6NdrtzN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4041bab9-abc4-48a8-b96a-ecb901405021"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content extracted for https://insights.blackcoffer.com/rising-it-cities-and-its-impact-on-the-economy-environment-infrastructure-and-city-life-by-the-year-2040-2/ with URL_ID blackassign0001\n",
            "Content extracted for https://insights.blackcoffer.com/rising-it-cities-and-their-impact-on-the-economy-environment-infrastructure-and-city-life-in-future/ with URL_ID blackassign0002\n",
            "Content extracted for https://insights.blackcoffer.com/internet-demands-evolution-communication-impact-and-2035s-alternative-pathways/ with URL_ID blackassign0003\n",
            "Content extracted for https://insights.blackcoffer.com/rise-of-cybercrime-and-its-effect-in-upcoming-future/ with URL_ID blackassign0004\n",
            "Content extracted for https://insights.blackcoffer.com/ott-platform-and-its-impact-on-the-entertainment-industry-in-future/ with URL_ID blackassign0005\n",
            "Content extracted for https://insights.blackcoffer.com/the-rise-of-the-ott-platform-and-its-impact-on-the-entertainment-industry-by-2040/ with URL_ID blackassign0006\n",
            "Content extracted for https://insights.blackcoffer.com/rise-of-cyber-crime-and-its-effects/ with URL_ID blackassign0007\n",
            "Content extracted for https://insights.blackcoffer.com/rise-of-internet-demand-and-its-impact-on-communications-and-alternatives-by-the-year-2035-2/ with URL_ID blackassign0008\n",
            "Content extracted for https://insights.blackcoffer.com/rise-of-cybercrime-and-its-effect-by-the-year-2040-2/ with URL_ID blackassign0009\n",
            "Content extracted for https://insights.blackcoffer.com/rise-of-cybercrime-and-its-effect-by-the-year-2040/ with URL_ID blackassign0010\n",
            "Content extracted for https://insights.blackcoffer.com/rise-of-internet-demand-and-its-impact-on-communications-and-alternatives-by-the-year-2035/ with URL_ID blackassign0011\n",
            "Content extracted for https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-3-2/ with URL_ID blackassign0012\n",
            "Content extracted for https://insights.blackcoffer.com/rise-of-e-health-and-its-impact-on-humans-by-the-year-2030/ with URL_ID blackassign0013\n",
            "No content extracted for https://insights.blackcoffer.com/rise-of-e-health-and-its-imapct-on-humans-by-the-year-2030-2/\n",
            "Content extracted for https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-2/ with URL_ID blackassign0015\n",
            "Content extracted for https://insights.blackcoffer.com/rise-of-telemedicine-and-its-impact-on-livelihood-by-2040-2-2/ with URL_ID blackassign0016\n",
            "Content extracted for https://insights.blackcoffer.com/rise-of-chatbots-and-its-impact-on-customer-support-by-the-year-2040/ with URL_ID blackassign0017\n",
            "Content extracted for https://insights.blackcoffer.com/rise-of-e-health-and-its-imapct-on-humans-by-the-year-2030/ with URL_ID blackassign0018\n",
            "Content extracted for https://insights.blackcoffer.com/how-does-marketing-influence-businesses-and-consumers/ with URL_ID blackassign0019\n",
            "No content extracted for https://insights.blackcoffer.com/how-advertisement-increase-your-market-value/\n",
            "Content extracted for https://insights.blackcoffer.com/negative-effects-of-marketing-on-society/ with URL_ID blackassign0021\n",
            "Content extracted for https://insights.blackcoffer.com/how-advertisement-marketing-affects-business/ with URL_ID blackassign0022\n",
            "Content extracted for https://insights.blackcoffer.com/rising-it-cities-will-impact-the-economy-environment-infrastructure-and-city-life-by-the-year-2035/ with URL_ID blackassign0023\n",
            "Content extracted for https://insights.blackcoffer.com/rise-of-ott-platform-and-its-impact-on-entertainment-industry-by-the-year-2030/ with URL_ID blackassign0024\n",
            "Content extracted for https://insights.blackcoffer.com/rise-of-electric-vehicles-and-its-impact-on-livelihood-by-2040/ with URL_ID blackassign0025\n",
            "Content extracted for https://insights.blackcoffer.com/rise-of-electric-vehicle-and-its-impact-on-livelihood-by-the-year-2040/ with URL_ID blackassign0026\n",
            "Content extracted for https://insights.blackcoffer.com/oil-prices-by-the-year-2040-and-how-it-will-impact-the-world-economy/ with URL_ID blackassign0027\n",
            "Content extracted for https://insights.blackcoffer.com/an-outlook-of-healthcare-by-the-year-2040-and-how-it-will-impact-human-lives/ with URL_ID blackassign0028\n",
            "No content extracted for https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/\n",
            "Content extracted for https://insights.blackcoffer.com/what-if-the-creation-is-taking-over-the-creator/ with URL_ID blackassign0030\n",
            "Content extracted for https://insights.blackcoffer.com/what-jobs-will-robots-take-from-humans-in-the-future/ with URL_ID blackassign0031\n",
            "Content extracted for https://insights.blackcoffer.com/will-machine-replace-the-human-in-the-future-of-work/ with URL_ID blackassign0032\n",
            "Content extracted for https://insights.blackcoffer.com/will-ai-replace-us-or-work-with-us/ with URL_ID blackassign0033\n",
            "Content extracted for https://insights.blackcoffer.com/man-and-machines-together-machines-are-more-diligent-than-humans-blackcoffe/ with URL_ID blackassign0034\n",
            "Content extracted for https://insights.blackcoffer.com/in-future-or-in-upcoming-years-humans-and-machines-are-going-to-work-together-in-every-field-of-work/ with URL_ID blackassign0035\n",
            "No content extracted for https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "Content extracted for https://insights.blackcoffer.com/how-machine-learning-will-affect-your-business/ with URL_ID blackassign0037\n",
            "Content extracted for https://insights.blackcoffer.com/deep-learning-impact-on-areas-of-e-learning/ with URL_ID blackassign0038\n",
            "Content extracted for https://insights.blackcoffer.com/how-to-protect-future-data-and-its-privacy-blackcoffer/ with URL_ID blackassign0039\n",
            "Content extracted for https://insights.blackcoffer.com/how-machines-ai-automations-and-robo-human-are-effective-in-finance-and-banking/ with URL_ID blackassign0040\n",
            "Content extracted for https://insights.blackcoffer.com/ai-human-robotics-machine-future-planet-blackcoffer-thinking-jobs-workplace/ with URL_ID blackassign0041\n",
            "Content extracted for https://insights.blackcoffer.com/how-ai-will-change-the-world-blackcoffer/ with URL_ID blackassign0042\n",
            "No content extracted for https://insights.blackcoffer.com/future-of-work-how-ai-has-entered-the-workplace/\n",
            "Content extracted for https://insights.blackcoffer.com/ai-tool-alexa-google-assistant-finance-banking-tool-future/ with URL_ID blackassign0044\n",
            "Content extracted for https://insights.blackcoffer.com/ai-healthcare-revolution-ml-technology-algorithm-google-analytics-industrialrevolution/ with URL_ID blackassign0045\n",
            "Content extracted for https://insights.blackcoffer.com/all-you-need-to-know-about-online-marketing/ with URL_ID blackassign0046\n",
            "Content extracted for https://insights.blackcoffer.com/evolution-of-advertising-industry/ with URL_ID blackassign0047\n",
            "Content extracted for https://insights.blackcoffer.com/how-data-analytics-can-help-your-business-respond-to-the-impact-of-covid-19/ with URL_ID blackassign0048\n",
            "No content extracted for https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "Content extracted for https://insights.blackcoffer.com/environmental-impact-of-the-covid-19-pandemic-lesson-for-the-future/ with URL_ID blackassign0050\n",
            "Content extracted for https://insights.blackcoffer.com/how-data-analytics-and-ai-are-used-to-halt-the-covid-19-pandemic/ with URL_ID blackassign0051\n",
            "Content extracted for https://insights.blackcoffer.com/difference-between-artificial-intelligence-machine-learning-statistics-and-data-mining/ with URL_ID blackassign0052\n",
            "Content extracted for https://insights.blackcoffer.com/how-python-became-the-first-choice-for-data-science/ with URL_ID blackassign0053\n",
            "Content extracted for https://insights.blackcoffer.com/how-google-fit-measure-heart-and-respiratory-rates-using-a-phone/ with URL_ID blackassign0054\n",
            "Content extracted for https://insights.blackcoffer.com/what-is-the-future-of-mobile-apps/ with URL_ID blackassign0055\n",
            "Content extracted for https://insights.blackcoffer.com/impact-of-ai-in-health-and-medicine/ with URL_ID blackassign0056\n",
            "Content extracted for https://insights.blackcoffer.com/telemedicine-what-patients-like-and-dislike-about-it/ with URL_ID blackassign0057\n",
            "Content extracted for https://insights.blackcoffer.com/how-we-forecast-future-technologies/ with URL_ID blackassign0058\n",
            "Content extracted for https://insights.blackcoffer.com/can-robots-tackle-late-life-loneliness/ with URL_ID blackassign0059\n",
            "Content extracted for https://insights.blackcoffer.com/embedding-care-robots-into-society-socio-technical-considerations/ with URL_ID blackassign0060\n",
            "Content extracted for https://insights.blackcoffer.com/management-challenges-for-future-digitalization-of-healthcare-services/ with URL_ID blackassign0061\n",
            "Content extracted for https://insights.blackcoffer.com/are-we-any-closer-to-preventing-a-nuclear-holocaust/ with URL_ID blackassign0062\n",
            "Content extracted for https://insights.blackcoffer.com/will-technology-eliminate-the-need-for-animal-testing-in-drug-development/ with URL_ID blackassign0063\n",
            "Content extracted for https://insights.blackcoffer.com/will-we-ever-understand-the-nature-of-consciousness/ with URL_ID blackassign0064\n",
            "Content extracted for https://insights.blackcoffer.com/will-we-ever-colonize-outer-space/ with URL_ID blackassign0065\n",
            "Content extracted for https://insights.blackcoffer.com/what-is-the-chance-homo-sapiens-will-survive-for-the-next-500-years/ with URL_ID blackassign0066\n",
            "Content extracted for https://insights.blackcoffer.com/why-does-your-business-need-a-chatbot/ with URL_ID blackassign0067\n",
            "Content extracted for https://insights.blackcoffer.com/how-you-lead-a-project-or-a-team-without-any-technical-expertise/ with URL_ID blackassign0068\n",
            "Content extracted for https://insights.blackcoffer.com/can-you-be-great-leader-without-technical-expertise/ with URL_ID blackassign0069\n",
            "Content extracted for https://insights.blackcoffer.com/how-does-artificial-intelligence-affect-the-environment/ with URL_ID blackassign0070\n",
            "Content extracted for https://insights.blackcoffer.com/how-to-overcome-your-fear-of-making-mistakes-2/ with URL_ID blackassign0071\n",
            "Content extracted for https://insights.blackcoffer.com/is-perfection-the-greatest-enemy-of-productivity/ with URL_ID blackassign0072\n",
            "Content extracted for https://insights.blackcoffer.com/global-financial-crisis-2008-causes-effects-and-its-solution/ with URL_ID blackassign0073\n",
            "Content extracted for https://insights.blackcoffer.com/gender-diversity-and-equality-in-the-tech-industry/ with URL_ID blackassign0074\n",
            "Content extracted for https://insights.blackcoffer.com/how-to-overcome-your-fear-of-making-mistakes/ with URL_ID blackassign0075\n",
            "Content extracted for https://insights.blackcoffer.com/how-small-business-can-survive-the-coronavirus-crisis/ with URL_ID blackassign0076\n",
            "Content extracted for https://insights.blackcoffer.com/impacts-of-covid-19-on-vegetable-vendors-and-food-stalls/ with URL_ID blackassign0077\n",
            "Content extracted for https://insights.blackcoffer.com/impacts-of-covid-19-on-vegetable-vendors/ with URL_ID blackassign0078\n",
            "Content extracted for https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-tourism-aviation-industries/ with URL_ID blackassign0079\n",
            "Content extracted for https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-sports-events-around-the-world/ with URL_ID blackassign0080\n",
            "Content extracted for https://insights.blackcoffer.com/changing-landscape-and-emerging-trends-in-the-indian-it-ites-industry/ with URL_ID blackassign0081\n",
            "Content extracted for https://insights.blackcoffer.com/online-gaming-adolescent-online-gaming-effects-demotivated-depression-musculoskeletal-and-psychosomatic-symptoms/ with URL_ID blackassign0082\n",
            "No content extracted for https://insights.blackcoffer.com/human-rights-outlook/\n",
            "No content extracted for https://insights.blackcoffer.com/how-voice-search-makes-your-business-a-successful-business/\n",
            "Content extracted for https://insights.blackcoffer.com/how-the-covid-19-crisis-is-redefining-jobs-and-services/ with URL_ID blackassign0085\n",
            "Content extracted for https://insights.blackcoffer.com/how-to-increase-social-media-engagement-for-marketers/ with URL_ID blackassign0086\n",
            "Content extracted for https://insights.blackcoffer.com/impacts-of-covid-19-on-streets-sides-food-stalls/ with URL_ID blackassign0087\n",
            "Content extracted for https://insights.blackcoffer.com/coronavirus-impact-on-energy-markets-2/ with URL_ID blackassign0088\n",
            "Content extracted for https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-5/ with URL_ID blackassign0089\n",
            "Content extracted for https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis-4/ with URL_ID blackassign0090\n",
            "Content extracted for https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work-2/ with URL_ID blackassign0091\n",
            "No content extracted for https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work-3/\n",
            "Content extracted for https://insights.blackcoffer.com/travel-and-tourism-outlook/ with URL_ID blackassign0093\n",
            "Content extracted for https://insights.blackcoffer.com/gaming-disorder-and-effects-of-gaming-on-health/ with URL_ID blackassign0094\n",
            "Content extracted for https://insights.blackcoffer.com/what-is-the-repercussion-of-the-environment-due-to-the-covid-19-pandemic-situation/ with URL_ID blackassign0095\n",
            "Content extracted for https://insights.blackcoffer.com/what-is-the-repercussion-of-the-environment-due-to-the-covid-19-pandemic-situation-2/ with URL_ID blackassign0096\n",
            "Content extracted for https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-office-space-and-co-working-industries/ with URL_ID blackassign0097\n",
            "Content extracted for https://insights.blackcoffer.com/contribution-of-handicrafts-visual-arts-literature-in-the-indian-economy/ with URL_ID blackassign0098\n",
            "No content extracted for https://insights.blackcoffer.com/how-covid-19-is-impacting-payment-preferences/\n",
            "No content extracted for https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-2/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Sentiment Analysis"
      ],
      "metadata": {
        "id": "oyPTwNGjbwTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "Imn7cB0Ay6lz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0b2827-0632-4e14-9a42-f7d2ca0ca51c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwunGq4F4O2X",
        "outputId": "f0abab5e-608d-4931-8500-ccdaff46f739"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_stopwords(words, stopwords):\n",
        "    return [word for word in words if word.lower() not in stopwords]\n",
        "\n",
        "def calculate_positive_score(text, positive_words):\n",
        "    return sum(1 for word in text.split() if word.lower() in positive_words)\n",
        "\n",
        "def calculate_negative_score(text, negative_words):\n",
        "    return sum(1 for word in text.split() if word.lower() in negative_words)\n",
        "\n",
        "def calculate_polarity_score(positive_score, negative_score):\n",
        "    denominator = (positive_score + negative_score) + 1e-6\n",
        "    return (positive_score - negative_score) / denominator\n",
        "\n",
        "def calculate_subjectivity_score(positive_score, negative_score, total_words):\n",
        "    denominator = (total_words + 1e-6)\n",
        "    return (positive_score + negative_score) / denominator\n",
        "\n",
        "def average_word_length(words):\n",
        "    total_length = sum(len(word) for word in words)\n",
        "    return total_length / len(words) if len(words) > 0 else 0\n"
      ],
      "metadata": {
        "id": "YYUv3y3YPW9w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_stopwords_from_files(stopwords_files, encoding='utf-8'):\n",
        "    combined_stopwords = set()\n",
        "    for file_path in stopwords_files:\n",
        "        with open(file_path, 'r', encoding=encoding, errors='ignore') as file:\n",
        "            stopwords_list = set(file.read().splitlines())\n",
        "        combined_stopwords.update(stopwords_list)\n",
        "    return combined_stopwords\n",
        "\n",
        "def load_word_dictionary(file_path, encoding='latin-1'):\n",
        "    with open(file_path, 'r', encoding=encoding, errors='ignore') as file:\n",
        "        word_list = file.read().splitlines()\n",
        "    return set(word_list)\n"
      ],
      "metadata": {
        "id": "sf5Gj99n89dV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_files = ['StopWords_Auditor.txt', 'StopWords_Currencies.txt', 'StopWords_DatesandNumbers.txt',\n",
        "                   'StopWords_Generic.txt', 'StopWords_GenericLong.txt', 'StopWords_Geographic.txt', 'StopWords_Names.txt']\n",
        "\n",
        "combined_stopwords = load_stopwords_from_files(stopwords_files)"
      ],
      "metadata": {
        "id": "kO3Xw6O59HKH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_words_file = 'positive-words.txt'\n",
        "negative_words_file = 'negative-words.txt'\n",
        "positive_words = load_word_dictionary(positive_words_file)\n",
        "negative_words = load_word_dictionary(negative_words_file)"
      ],
      "metadata": {
        "id": "iEZoYKz19PAF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_folder = 'output_folder/'"
      ],
      "metadata": {
        "id": "3mA3da6MUyEu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import cmudict\n",
        "nltk.download('punkt')\n",
        "nltk.download('cmudict')\n",
        "import re\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZglMeRGhMwu",
        "outputId": "3b6e04a1-0e11-4334-940c-102ce765bad9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "id": "wD7ayHDcPeWU",
        "outputId": "6cbf1adf-2acd-4bf4-8214-85d91ea5c54c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m802.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.14.0 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "output_df = pd.DataFrame(columns=[\n",
        "    'URL_ID',\n",
        "    'URL',\n",
        "    'POSITIVE SCORE',\n",
        "    'NEGATIVE SCORE',\n",
        "    'POLARITY SCORE',\n",
        "    'SUBJECTIVITY SCORE',\n",
        "    'AVG SENTENCE LENGTH',\n",
        "    'PERCENTAGE OF COMPLEX WORDS',\n",
        "    'FOG INDEX',\n",
        "    'AVG NUMBER OF WORDS PER SENTENCE',\n",
        "    'COMPLEX WORD COUNT',\n",
        "    'WORD COUNTt',\n",
        "    'SYLLABLE PER WORD',\n",
        "    'PERSONAL PRONOUNS',\n",
        "    'AVG WORD LENGTH'\n",
        "])"
      ],
      "metadata": {
        "id": "MaTAV0W4P-Ke"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textstat import syllable_count"
      ],
      "metadata": {
        "id": "tJD7YxTmQeKd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install natsort"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqrgBVP8NB3T",
        "outputId": "c3bf149e-8fc0-4f7d-e8f0-c86896566510"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (8.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from natsort import natsorted\n",
        "\n",
        "\n",
        "for filename in natsorted(os.listdir(output_folder)):\n",
        "    if filename.endswith('.txt'):\n",
        "        file_path = os.path.join(output_folder, filename)\n",
        "        print(f\"Processing file: {filename}\")\n",
        "        url_id, _ = os.path.splitext(filename)\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            article_text = file.read()\n",
        "\n",
        "            #sentiment analysis\n",
        "            tokens = word_tokenize(article_text)\n",
        "            filtered_tokens = filter_stopwords(tokens, combined_stopwords)\n",
        "            filtered_text = ' '.join(filtered_tokens)\n",
        "            positive_score = calculate_positive_score(filtered_text, positive_words)\n",
        "            negative_score = calculate_negative_score(filtered_text, negative_words)\n",
        "            total_words = len(filtered_tokens)\n",
        "            polarity_score = calculate_polarity_score(positive_score, negative_score)\n",
        "            subjectivity_score = calculate_subjectivity_score(positive_score, negative_score, total_words)\n",
        "\n",
        "            #additional variables\n",
        "            sentences = sent_tokenize(article_text)\n",
        "            avg_sentence_length = len(tokens) / len(sentences)\n",
        "\n",
        "            #textstat for syllable counting\n",
        "            complex_words = [word for word in tokens if syllable_count(word) > 2]\n",
        "\n",
        "            percentage_complex_words = (len(complex_words) / len(tokens)) * 100\n",
        "            fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "            avg_number_words_per_sentence = total_words / len(sentences)\n",
        "            complex_word_count = len(complex_words)\n",
        "            word_count = len(tokens)\n",
        "            syllable_per_word = sum(syllable_count(word) for word in tokens) / len(tokens)\n",
        "\n",
        "            #personal pronouns count using regex\n",
        "            personal_pronouns_count = len(re.findall(r'\\b(I|we|my|ours|us)\\b', article_text, flags=re.IGNORECASE))\n",
        "\n",
        "            avg_word_length = average_word_length(tokens)\n",
        "\n",
        "\n",
        "            url = df[df['URL_ID'] == url_id]['URL'].iloc[0]\n",
        "\n",
        "            row_data = {\n",
        "                'URL_ID': url_id,\n",
        "                'URL': url,\n",
        "                'POSITIVE SCORE': positive_score,\n",
        "                'NEGATIVE SCORE': negative_score,\n",
        "                'POLARITY SCORE': polarity_score,\n",
        "                'SUBJECTIVITY SCORE': subjectivity_score,\n",
        "                'AVG SENTENCE LENGTH': avg_sentence_length,\n",
        "                'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
        "                'FOG INDEX': fog_index,\n",
        "                'AVG NUMBER OF WORDS PER SENTENCE': avg_number_words_per_sentence,\n",
        "                'COMPLEX WORD COUNT': complex_word_count,\n",
        "                'WORD COUNTt': word_count,\n",
        "                'SYLLABLE PER WORD': syllable_per_word,\n",
        "                'PERSONAL PRONOUNS': personal_pronouns_count,\n",
        "                'AVG WORD LENGTH': avg_word_length\n",
        "}\n",
        "\n",
        "            output_df = pd.concat([output_df, pd.DataFrame([row_data])], ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "output_df.to_csv('output_results1.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYhZa4cRNB5F",
        "outputId": "7198b500-f468-4c77-e998-0ca01fbb232e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: blackassign0001.txt\n",
            "Processing file: blackassign0002.txt\n",
            "Processing file: blackassign0003.txt\n",
            "Processing file: blackassign0004.txt\n",
            "Processing file: blackassign0005.txt\n",
            "Processing file: blackassign0006.txt\n",
            "Processing file: blackassign0007.txt\n",
            "Processing file: blackassign0008.txt\n",
            "Processing file: blackassign0009.txt\n",
            "Processing file: blackassign0010.txt\n",
            "Processing file: blackassign0011.txt\n",
            "Processing file: blackassign0012.txt\n",
            "Processing file: blackassign0013.txt\n",
            "Processing file: blackassign0015.txt\n",
            "Processing file: blackassign0016.txt\n",
            "Processing file: blackassign0017.txt\n",
            "Processing file: blackassign0018.txt\n",
            "Processing file: blackassign0019.txt\n",
            "Processing file: blackassign0021.txt\n",
            "Processing file: blackassign0022.txt\n",
            "Processing file: blackassign0023.txt\n",
            "Processing file: blackassign0024.txt\n",
            "Processing file: blackassign0025.txt\n",
            "Processing file: blackassign0026.txt\n",
            "Processing file: blackassign0027.txt\n",
            "Processing file: blackassign0028.txt\n",
            "Processing file: blackassign0030.txt\n",
            "Processing file: blackassign0031.txt\n",
            "Processing file: blackassign0032.txt\n",
            "Processing file: blackassign0033.txt\n",
            "Processing file: blackassign0034.txt\n",
            "Processing file: blackassign0035.txt\n",
            "Processing file: blackassign0037.txt\n",
            "Processing file: blackassign0038.txt\n",
            "Processing file: blackassign0039.txt\n",
            "Processing file: blackassign0040.txt\n",
            "Processing file: blackassign0041.txt\n",
            "Processing file: blackassign0042.txt\n",
            "Processing file: blackassign0044.txt\n",
            "Processing file: blackassign0045.txt\n",
            "Processing file: blackassign0046.txt\n",
            "Processing file: blackassign0047.txt\n",
            "Processing file: blackassign0048.txt\n",
            "Processing file: blackassign0050.txt\n",
            "Processing file: blackassign0051.txt\n",
            "Processing file: blackassign0052.txt\n",
            "Processing file: blackassign0053.txt\n",
            "Processing file: blackassign0054.txt\n",
            "Processing file: blackassign0055.txt\n",
            "Processing file: blackassign0056.txt\n",
            "Processing file: blackassign0057.txt\n",
            "Processing file: blackassign0058.txt\n",
            "Processing file: blackassign0059.txt\n",
            "Processing file: blackassign0060.txt\n",
            "Processing file: blackassign0061.txt\n",
            "Processing file: blackassign0062.txt\n",
            "Processing file: blackassign0063.txt\n",
            "Processing file: blackassign0064.txt\n",
            "Processing file: blackassign0065.txt\n",
            "Processing file: blackassign0066.txt\n",
            "Processing file: blackassign0067.txt\n",
            "Processing file: blackassign0068.txt\n",
            "Processing file: blackassign0069.txt\n",
            "Processing file: blackassign0070.txt\n",
            "Processing file: blackassign0071.txt\n",
            "Processing file: blackassign0072.txt\n",
            "Processing file: blackassign0073.txt\n",
            "Processing file: blackassign0074.txt\n",
            "Processing file: blackassign0075.txt\n",
            "Processing file: blackassign0076.txt\n",
            "Processing file: blackassign0077.txt\n",
            "Processing file: blackassign0078.txt\n",
            "Processing file: blackassign0079.txt\n",
            "Processing file: blackassign0080.txt\n",
            "Processing file: blackassign0081.txt\n",
            "Processing file: blackassign0082.txt\n",
            "Processing file: blackassign0085.txt\n",
            "Processing file: blackassign0086.txt\n",
            "Processing file: blackassign0087.txt\n",
            "Processing file: blackassign0088.txt\n",
            "Processing file: blackassign0089.txt\n",
            "Processing file: blackassign0090.txt\n",
            "Processing file: blackassign0091.txt\n",
            "Processing file: blackassign0093.txt\n",
            "Processing file: blackassign0094.txt\n",
            "Processing file: blackassign0095.txt\n",
            "Processing file: blackassign0096.txt\n",
            "Processing file: blackassign0097.txt\n",
            "Processing file: blackassign0098.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "output_csv_path = 'output_results1.csv'\n",
        "\n",
        "files.download(output_csv_path)\n"
      ],
      "metadata": {
        "id": "xZQpksCMR9bE",
        "outputId": "7fa425cd-c5a2-4007-ae27-31e837e6e3fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a4c7c374-3076-4f2d-9dfd-ac850ae0d5aa\", \"output_results1.csv\", 24302)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_FX7LdhHSk8n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}