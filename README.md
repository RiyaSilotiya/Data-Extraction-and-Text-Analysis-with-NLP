# Data-Extraction-and-Text-Analysis-with-NLP

## Approach to the Solution:

## 1. Data Extraction:
Utilized Python for web data extraction and analysis.
Employed Beautiful Soup library for effective web scraping.

## 2. Sentiment Analysis:
Loaded stop words from specified files for filtering.
Utilized NLTK for tokenization and natural language processing.
Employed pre-existing sentiment dictionaries for positive and negative scores.
Calculated polarity and subjectivity scores based on obtained sentiment scores.

## 3. Additional Variables Calculation:

Computed average sentence length, percentage of complex words, and Fog Index for each document.
Measured personal pronouns count and calculated average word length.

## 4. Instructions:
Considered all instructions from the text analysis file for variable calculations.
Extracted information like URL and URL_ID to associate sentiment scores with specific documents.
Running the .py File:
1. Environment Setup:
Ensure Python is installed on the system.
Install required libraries.

3. Execution:
Place the provided .py file in the desired directory.
Upload the input file and other necessary files (Stopwords and MasterDictionary) in the same directory as the .py file.
Run the script.

4. Output:
The output is saved in a CSV file named output_results1.csv.
The CSV file contains sentiment scores, additional variables, and relevant information for each URL.

Dependencies:
Python,
NLTK (Natural Language Toolkit),
Pandas,
BeautifulSoup
,Requests
,Os
,Nltk.tokenize
For detailed instructions and code, please refer to the provided .py file and associated documentation.





